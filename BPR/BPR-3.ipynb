{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BPR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZvY6IEHAUkq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70af020e-e535-44ca-a201-d333469bb4c9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL0HAycqAX84",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "bad0e13e-16c3-4e4b-c2e9-4033fadd5d01"
      },
      "source": [
        "!pip install implicit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import trange\n",
        "%cd /content/drive/My Drive/Colab Notebooks/Thesis"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: implicit in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from implicit) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from implicit) (1.18.1)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.6/dist-packages (from implicit) (1.4.1)\n",
            "/content/drive/My Drive/Colab Notebooks/Thesis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ2i7IzoAPUb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "9a13e209-449c-4ab5-fe21-d33c26513e06"
      },
      "source": [
        "pip install -r final/bpr-master/requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.18.1 in /usr/local/lib/python3.6/dist-packages (from -r final/bpr-master/requirements.txt (line 1)) (1.18.1)\n",
            "Requirement already satisfied: pandas==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r final/bpr-master/requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r final/bpr-master/requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: torchvision==0.5.0 in /usr/local/lib/python3.6/dist-packages (from -r final/bpr-master/requirements.txt (line 4)) (0.5.0)\n",
            "Requirement already satisfied: pytest==5.3.5 in /usr/local/lib/python3.6/dist-packages (from -r final/bpr-master/requirements.txt (line 5)) (5.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.1->-r final/bpr-master/requirements.txt (line 2)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.1->-r final/bpr-master/requirements.txt (line 2)) (2018.9)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0->-r final/bpr-master/requirements.txt (line 4)) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0->-r final/bpr-master/requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->-r final/bpr-master/requirements.txt (line 5)) (20.4)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->-r final/bpr-master/requirements.txt (line 5)) (8.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->-r final/bpr-master/requirements.txt (line 5)) (0.2.5)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->-r final/bpr-master/requirements.txt (line 5)) (1.9.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->-r final/bpr-master/requirements.txt (line 5)) (19.3.0)\n",
            "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->-r final/bpr-master/requirements.txt (line 5)) (0.13.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->-r final/bpr-master/requirements.txt (line 5)) (1.7.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pytest==5.3.5->-r final/bpr-master/requirements.txt (line 5)) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest==5.3.5->-r final/bpr-master/requirements.txt (line 5)) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT_TPDIxALXZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a5b46703-c209-4360-89dc-3fd4b1cdf43a"
      },
      "source": [
        "import os\n",
        "import gzip\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "import pprint\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class DatasetLoader(object):\n",
        "    def load(self):\n",
        "        \"\"\"Minimum condition for dataset:\n",
        "          * All users must have at least one item record.\n",
        "          * All items must have at least one user record.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Hetrec(DatasetLoader):\n",
        "    def __init__(self, data_dir):\n",
        "        self.fpath = os.path.join('/content/drive/My Drive/Colab Notebooks/Thesis/final/hetrec2011-lastfm-2k', 'user_artists.dat')\n",
        "\n",
        "    def load(self):\n",
        "        # Load data\n",
        "        df = pd.read_csv(self.fpath,\n",
        "                         sep=\"\\s+\",\n",
        "                         engine='python',\n",
        "                         names=['userID', 'itemID', 'weight'])\n",
        "        # TODO: Remove negative rating?\n",
        "        # df = df[df['rate'] >= 3]\n",
        "        return df\n",
        "\n",
        "\n",
        "def convert_unique_idx(df, column_name):\n",
        "    column_dict = {x: i for i, x in enumerate(df[column_name].unique())}\n",
        "    # print(column_dict)\n",
        "    df[column_name] = df[column_name].apply(column_dict.get)\n",
        "    # df[column_name] = ''.join(map(str,df[column_name])) #converting the list into string\n",
        "    # df[column_name] = np.float(df[column_name])\n",
        "    # df[column_name] = df[column_name].astype('float')\n",
        "    # df[column_name] = df[column_name].astype('int64')\n",
        "    df[column_name] = round(df[column_name])\n",
        "    # assert df[column_name].min() == 0\n",
        "    # assert df[column_name].max() == len(column_dict) - 1\n",
        "    return df, column_dict\n",
        "\n",
        "\n",
        "def create_user_list(df, user_size):\n",
        "    user_list = [list() for u in range(user_size)]\n",
        "    for row in df.itertuples():\n",
        "        user_list[row.userID].append(row.itemID)\n",
        "    return user_list\n",
        "\n",
        "\n",
        "def split_train_test(df, user_size, test_size=0.25, time_order=False):\n",
        "    \"\"\"Split a dataset into `train_user_list` and `test_user_list`.\n",
        "    Because it needs `user_list` for splitting dataset as `time_order` is set,\n",
        "    Returning `user_list` data structure will be a good choice.\"\"\"\n",
        "    # TODO: Handle duplicated items\n",
        "    if not time_order:\n",
        "        test_idx = np.random.choice(len(df), size=int(len(df)*test_size))\n",
        "        train_idx = list(set(range(len(df))) - set(test_idx))\n",
        "        test_df = df.loc[test_idx].reset_index(drop=True)\n",
        "        train_df = df.loc[train_idx].reset_index(drop=True)\n",
        "        test_user_list = create_user_list(test_df, user_size)\n",
        "        train_user_list = create_user_list(train_df, user_size)\n",
        "    else:\n",
        "        total_user_list = create_user_list(df, user_size)\n",
        "        train_user_list = [None] * len(user_list)\n",
        "        test_user_list = [None] * len(user_list)\n",
        "        for user, item_list in enumerate(total_user_list):\n",
        "            # Choose latest item\n",
        "            item_list = sorted(item_list, key=lambda x: x[0])\n",
        "            # Split item\n",
        "            test_item = item_list[math.ceil(len(item_list)*(1-test_size)):]\n",
        "            train_item = item_list[:math.ceil(len(item_list)*(1-test_size))]\n",
        "            # Register to each user list\n",
        "            test_user_list[user] = test_item\n",
        "            train_user_list[user] = train_item\n",
        "    # # Remove time --> dont have time\n",
        "    # test_user_list = [list(map(lambda x: x[1], l)) for l in test_user_list]\n",
        "    # train_user_list = [list(map(lambda x: x[1], l)) for l in train_user_list]\n",
        "    return train_user_list, test_user_list\n",
        "\n",
        "\n",
        "def create_pair(user_list):\n",
        "    pair = []\n",
        "    for user, item_list in enumerate(user_list):\n",
        "        pair.extend([(user, item) for item in item_list])\n",
        "    return pair\n",
        "\n",
        "\n",
        "def main():\n",
        "    dataset = \"hetrec\"\n",
        "    data_dir = \"hetrec2011-lastfm-2k\"\n",
        "    output_data = os.path.join('preprocessed', 'hetrec.pickle')\n",
        "    test_size = 0.25\n",
        "    time_order = False\n",
        "    \n",
        "    if dataset == 'ml-1m':\n",
        "        df = MovieLens1M(data_dir).load()\n",
        "    elif dataset == 'hetrec':\n",
        "        df = Hetrec(data_dir).load()\n",
        "    elif dataset == 'ml-20m':\n",
        "        df = MovieLens20M(data_dir).load()\n",
        "    elif dataset == 'amazon-beauty':\n",
        "        df = AmazonBeauty(data_dir).load()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    df, user_mapping = convert_unique_idx(df, 'userID')\n",
        "    df, item_mapping = convert_unique_idx(df, 'itemID')\n",
        "    print('Complete assigning unique index to user and item')\n",
        "\n",
        "    user_size = len(df['userID'].unique())\n",
        "    item_size = len(df['itemID'].unique())\n",
        "\n",
        "    train_user_list, test_user_list = split_train_test(df,\n",
        "                                                       user_size,\n",
        "                                                       test_size=test_size,\n",
        "                                                       time_order=time_order)\n",
        "    print('Complete spliting items for training and testing')\n",
        "\n",
        "    train_pair = create_pair(train_user_list)\n",
        "    print('Complete creating pair')\n",
        "\n",
        "    dataset = {'user_size': user_size, 'item_size': item_size, \n",
        "               'user_mapping': user_mapping, 'item_mapping': item_mapping,\n",
        "               'train_user_list': train_user_list, 'test_user_list': test_user_list,\n",
        "               'train_pair': train_pair}\n",
        "    dirname = os.path.dirname(os.path.abspath(output_data))\n",
        "    os.makedirs(dirname, exist_ok=True)\n",
        "    with open(output_data, 'wb') as f:\n",
        "        pickle.dump(dataset, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "    # # Parse argument\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument('--dataset',\n",
        "    #                     choices=['ml-1m', 'ml-20m', 'amazon-beauty', 'gowalla','hetrec'])\n",
        "    # parser.add_argument('--data_dir',\n",
        "    #                     type=str,\n",
        "    #                     default=os.path.join('data', 'hetrec'),\n",
        "    #                     help=\"File path for raw data\")\n",
        "    # parser.add_argument('--output_data',\n",
        "    #                     type=str,\n",
        "    #                     default=os.path.join('preprocessed', 'hetrec.pickle'),\n",
        "    #                     help=\"File path for preprocessed data\")\n",
        "    # parser.add_argument('--test_size',\n",
        "    #                     type=float,\n",
        "    #                     default=0.25,\n",
        "    #                     help=\"Proportion for training and testing split\")\n",
        "\n",
        "    # args = parser.parse_args()\n",
        "    # main(args)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Complete assigning unique index to user and item\n",
            "Complete spliting items for training and testing\n",
            "Complete creating pair\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv0htHEYAbCf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "954fbdcd-8b61-42e8-f5c7-7f13b8ee14aa"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import pickle\n",
        "import argparse\n",
        "from collections import deque\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch.utils.data import IterableDataset, DataLoader, get_worker_info\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "class TripletUniformPair(IterableDataset):\n",
        "    def __init__(self, num_item, user_list, pair, shuffle, num_epochs):\n",
        "        self.num_item = num_item\n",
        "        self.user_list = user_list\n",
        "        self.pair = pair\n",
        "        self.shuffle = shuffle\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "    def __iter__(self):\n",
        "        worker_info = get_worker_info()\n",
        "        # Shuffle per epoch\n",
        "        self.example_size = self.num_epochs * len(self.pair)\n",
        "        self.example_index_queue = deque([])\n",
        "        self.seed = 0\n",
        "        if worker_info is not None:\n",
        "            self.start_list_index = worker_info.id\n",
        "            self.num_workers = worker_info.num_workers\n",
        "            self.index = worker_info.id\n",
        "        else:\n",
        "            self.start_list_index = None\n",
        "            self.num_workers = 1\n",
        "            self.index = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.index >= self.example_size:\n",
        "            raise StopIteration\n",
        "        # If `example_index_queue` is used up, replenish this list.\n",
        "        while len(self.example_index_queue) == 0:\n",
        "            index_list = list(range(len(self.pair)))\n",
        "            if self.shuffle:\n",
        "                random.Random(self.seed).shuffle(index_list)\n",
        "                self.seed += 1\n",
        "            if self.start_list_index is not None:\n",
        "                index_list = index_list[self.start_list_index::self.num_workers]\n",
        "                # Calculate next start index\n",
        "                self.start_list_index = (self.start_list_index + (self.num_workers - (len(self.pair) % self.num_workers))) % self.num_workers\n",
        "            self.example_index_queue.extend(index_list)\n",
        "        result = self._example(self.example_index_queue.popleft())\n",
        "        self.index += self.num_workers\n",
        "        return result\n",
        "\n",
        "    def _example(self, idx):\n",
        "        u = self.pair[idx][0]\n",
        "        i = self.pair[idx][1]\n",
        "        j = np.random.randint(self.num_item)\n",
        "        while j in self.user_list[u]:\n",
        "            j = np.random.randint(self.num_item)\n",
        "        return u, i, j\n",
        "\n",
        "\n",
        "class BPR(nn.Module):\n",
        "    def __init__(self, user_size, item_size, dim, weight_decay):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.empty(user_size, dim))\n",
        "        self.H = nn.Parameter(torch.empty(item_size, dim))\n",
        "        nn.init.xavier_normal_(self.W.data)\n",
        "        nn.init.xavier_normal_(self.H.data)\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "    def forward(self, u, i, j):\n",
        "        \"\"\"Return loss value.\n",
        "        \n",
        "        Args:\n",
        "            u(torch.LongTensor): tensor stored user indexes. [batch_size,]\n",
        "            i(torch.LongTensor): tensor stored item indexes which is prefered by user. [batch_size,]\n",
        "            j(torch.LongTensor): tensor stored item indexes which is not prefered by user. [batch_size,]\n",
        "        \n",
        "        Returns:\n",
        "            torch.FloatTensor\n",
        "        \"\"\"\n",
        "        u = self.W[u, :]\n",
        "        i = self.H[i, :]\n",
        "        j = self.H[j, :]\n",
        "        x_ui = torch.mul(u, i).sum(dim=1)\n",
        "        x_uj = torch.mul(u, j).sum(dim=1)\n",
        "        x_uij = x_ui - x_uj\n",
        "        log_prob = F.logsigmoid(x_uij).sum()\n",
        "        regularization = self.weight_decay * (u.norm(dim=1).pow(2).sum() + i.norm(dim=1).pow(2).sum() + j.norm(dim=1).pow(2).sum())\n",
        "        return -log_prob + regularization\n",
        "\n",
        "    def recommend(self, u):\n",
        "        \"\"\"Return recommended item list given users.\n",
        "        Args:\n",
        "            u(torch.LongTensor): tensor stored user indexes. [batch_size,]\n",
        "        Returns:\n",
        "            pred(torch.LongTensor): recommended item list sorted by preference. [batch_size, item_size]\n",
        "        \"\"\"\n",
        "        u = self.W[u, :]\n",
        "        x_ui = torch.mm(u, self.H.t())\n",
        "        pred = torch.argsort(x_ui, dim=1)\n",
        "        return pred\n",
        "\n",
        "\n",
        "def precision_and_recall_k(user_emb, item_emb, train_user_list, test_user_list, klist, batch=512):\n",
        "    \"\"\"Compute precision at k using GPU.\n",
        "    Args:\n",
        "        user_emb (torch.Tensor): embedding for user [user_num, dim]\n",
        "        item_emb (torch.Tensor): embedding for item [item_num, dim]\n",
        "        train_user_list (list(set)):\n",
        "        test_user_list (list(set)):\n",
        "        k (list(int)):\n",
        "    Returns:\n",
        "        (torch.Tensor, torch.Tensor) Precision and recall at k\n",
        "    \"\"\"\n",
        "    # Calculate max k value\n",
        "    max_k = max(klist)\n",
        "\n",
        "    # Compute all pair of training and test record\n",
        "    result = None\n",
        "    for i in range(0, user_emb.shape[0], batch):\n",
        "        # Create already observed mask\n",
        "        mask = user_emb.new_ones([min([batch, user_emb.shape[0]-i]), item_emb.shape[0]])\n",
        "        for j in range(batch):\n",
        "            if i+j >= user_emb.shape[0]:\n",
        "                break\n",
        "            mask[j].scatter_(dim=0, index=torch.LongTensor(train_user_list[i+j]).cuda(), value=torch.tensor(0.0).cuda())\n",
        "        # Calculate prediction value\n",
        "        cur_result = torch.mm(user_emb[i:i+min(batch, user_emb.shape[0]-i), :], item_emb.t())\n",
        "        cur_result = torch.sigmoid(cur_result)\n",
        "        assert not torch.any(torch.isnan(cur_result))\n",
        "        # Make zero for already observed item\n",
        "        cur_result = torch.mul(mask, cur_result)\n",
        "        _, cur_result = torch.topk(cur_result, k=max_k, dim=1)\n",
        "        result = cur_result if result is None else torch.cat((result, cur_result), dim=0)\n",
        "\n",
        "    result = result.cpu()\n",
        "    # Sort indice and get test_pred_topk\n",
        "    precisions, recalls = [], []\n",
        "    for k in klist:\n",
        "        precision, recall = 0, 0\n",
        "        for i in range(user_emb.shape[0]):\n",
        "            test = set(test_user_list[i])\n",
        "            pred = set(result[i, :k].numpy().tolist())\n",
        "            val = len(test & pred)\n",
        "            precision += val / max([min([k, len(test)]), 1])\n",
        "            recall += val / max([len(test), 1])\n",
        "        precisions.append(precision / user_emb.shape[0])\n",
        "        recalls.append(recall / user_emb.shape[0])\n",
        "    return precisions, recalls\n",
        "\n",
        "def main(): \n",
        "    data = os.path.join('/content/drive/My Drive/Colab Notebooks/Thesis/preprocessed', 'hetrec.pickle')\n",
        "    seed = 0\n",
        "    dim = 100\n",
        "    lr = 1e-3\n",
        "    weight_decay = 0.025\n",
        "    n_epochs = 40\n",
        "    batch_size = 4096\n",
        "    print_every = 20\n",
        "    eval_every = 100\n",
        "    save_every = 100000\n",
        "    model = os.path.join('output', 'bpr.pt')\n",
        "    \n",
        "    # Initialize seed\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Load preprocess data\n",
        "    with open(data, 'rb') as f:\n",
        "        dataset = pickle.load(f)\n",
        "        user_size, item_size = dataset['user_size'], dataset['item_size']\n",
        "        train_user_list, test_user_list = dataset['train_user_list'], dataset['test_user_list']\n",
        "        train_pair = dataset['train_pair']\n",
        "    print('Load complete')\n",
        "\n",
        "    # Create dataset, model, optimizer\n",
        "    dataset = TripletUniformPair(item_size, train_user_list, train_pair, True, n_epochs)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, num_workers=16)\n",
        "    model = BPR(user_size, item_size, dim, weight_decay).cuda()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    writer = SummaryWriter()\n",
        "    start = time.time()\n",
        "    # Training\n",
        "    smooth_loss = 0\n",
        "    idx = 0\n",
        "    for u, i, j in loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(u, i, j)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        writer.add_scalar('train/loss', loss, idx)\n",
        "        smooth_loss = smooth_loss*0.99 + loss*0.01\n",
        "        if idx % print_every == (print_every - 1):\n",
        "            print('loss: %.4f' % smooth_loss)\n",
        "            # print(idx)\n",
        "            # print(idx % eval_every)\n",
        "            # print(eval_every - 1)\n",
        "        if idx % eval_every == (eval_every - 1):\n",
        "            plist, rlist = precision_and_recall_k(model.W.detach(),\n",
        "                                                    model.H.detach(),\n",
        "                                                    train_user_list,\n",
        "                                                    test_user_list,\n",
        "                                                    klist=[1,5,10])\n",
        "            print('P@1: %.4f, P@5: %.4f P@10: %.4f, R@1: %.4f, R@5: %.4f, R@10: %.4f' % (plist[0], plist[1], plist[2], rlist[0], rlist[1], rlist[2]))\n",
        "            \n",
        "            writer.add_scalars('eval', {'P@1': plist[0],\n",
        "                                                    'P@5': plist[1],\n",
        "                                                    'P@10': plist[2]}, idx)\n",
        "            writer.add_scalars('eval', {'R@1': rlist[0],\n",
        "                                                'R@5': rlist[1],\n",
        "                                                'R@10': rlist[2]}, idx)\n",
        "        if idx % save_every == (save_every - 1):\n",
        "            dirname = os.path.dirname(os.path.abspath(model))\n",
        "            os.makedirs(dirname, exist_ok=True)\n",
        "            torch.save(model.state_dict(), model)\n",
        "        idx += 1\n",
        "    print('time',time.time()-start)  \n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load complete\n",
            "loss: 518.8221\n",
            "loss: 941.4379\n",
            "loss: 1285.1345\n",
            "loss: 1560.8456\n",
            "loss: 1771.7299\n",
            "P@1: 0.2171, P@5: 0.1334 P@10: 0.1032, R@1: 0.0210, R@5: 0.0617, R@10: 0.0881\n",
            "loss: 1913.7683\n",
            "loss: 1984.9093\n",
            "loss: 1995.9729\n",
            "loss: 1973.0909\n",
            "loss: 1930.4454\n",
            "P@1: 0.2520, P@5: 0.1687 P@10: 0.1293, R@1: 0.0245, R@5: 0.0786, R@10: 0.1104\n",
            "loss: 1879.7168\n",
            "loss: 1825.8030\n",
            "loss: 1777.7896\n",
            "loss: 1731.8922\n",
            "loss: 1690.2333\n",
            "P@1: 0.1764, P@5: 0.1408 P@10: 0.1183, R@1: 0.0170, R@5: 0.0657, R@10: 0.1017\n",
            "loss: 1653.7018\n",
            "loss: 1619.6022\n",
            "loss: 1589.1387\n",
            "loss: 1560.9141\n",
            "loss: 1532.0055\n",
            "P@1: 0.0887, P@5: 0.0983 P@10: 0.0888, R@1: 0.0089, R@5: 0.0456, R@10: 0.0758\n",
            "loss: 1505.7972\n",
            "loss: 1483.5688\n",
            "loss: 1464.1344\n",
            "loss: 1443.6583\n",
            "loss: 1427.2758\n",
            "P@1: 0.0977, P@5: 0.0876 P@10: 0.0924, R@1: 0.0095, R@5: 0.0402, R@10: 0.0788\n",
            "loss: 1411.8478\n",
            "loss: 1397.9319\n",
            "loss: 1384.1697\n",
            "loss: 1372.7397\n",
            "loss: 1361.4570\n",
            "P@1: 0.1442, P@5: 0.1160 P@10: 0.1090, R@1: 0.0134, R@5: 0.0543, R@10: 0.0937\n",
            "loss: 1349.8633\n",
            "loss: 1341.0901\n",
            "loss: 1329.2126\n",
            "loss: 1319.6019\n",
            "loss: 1311.4448\n",
            "P@1: 0.1521, P@5: 0.1276 P@10: 0.1161, R@1: 0.0142, R@5: 0.0591, R@10: 0.0994\n",
            "loss: 1136.6865\n",
            "time 83.50560927391052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDIAtlV_J35I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}