{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BPR.ipynb","provenance":[],"authorship_tag":"ABX9TyN4LrRNBE0fC7II1QM3HcmJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"hZvY6IEHAUkq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592827446077,"user_tz":-120,"elapsed":934,"user":{"displayName":"Costanza Siani","photoUrl":"","userId":"11236584817392907357"}},"outputId":"8652d82b-4f22-4f86-c0a3-b18652ff9f9d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uL0HAycqAX84","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1592827450820,"user_tz":-120,"elapsed":4253,"user":{"displayName":"Costanza Siani","photoUrl":"","userId":"11236584817392907357"}},"outputId":"0728d3b5-a939-47e3-d99a-90f810135b0f"},"source":["!pip install implicit\n","import numpy as np\n","import pandas as pd\n","from tqdm import trange\n","%cd /content/drive/My Drive/Colab Notebooks/Thesis"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: implicit in /usr/local/lib/python3.6/dist-packages (0.4.2)\n","Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.6/dist-packages (from implicit) (1.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from implicit) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from implicit) (1.18.5)\n","/content/drive/My Drive/Colab Notebooks/Thesis\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CZ2i7IzoAPUb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":956},"executionInfo":{"status":"ok","timestamp":1592827612596,"user_tz":-120,"elapsed":160551,"user":{"displayName":"Costanza Siani","photoUrl":"","userId":"11236584817392907357"}},"outputId":"56c27cfb-d614-4b41-a956-37c2923c39f1"},"source":["pip install -r bpr-master/requirements.txt"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting numpy==1.18.1\n","  Using cached https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl\n","Collecting pandas==1.0.1\n","  Using cached https://files.pythonhosted.org/packages/08/ec/b5dd8cfb078380fb5ae9325771146bccd4e8cad2d3e4c72c7433010684eb/pandas-1.0.1-cp36-cp36m-manylinux1_x86_64.whl\n","Collecting torch==1.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n","\u001b[K     |████████████████████████████████| 753.4MB 25kB/s \n","\u001b[?25hCollecting torchvision==0.5.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n","\u001b[K     |████████████████████████████████| 4.0MB 34.5MB/s \n","\u001b[?25hCollecting pytest==5.3.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/c0/34033b2df7718b91c667bd259d5ce632ec3720198b7068c0ba6f6104ff89/pytest-5.3.5-py3-none-any.whl (235kB)\n","\u001b[K     |████████████████████████████████| 235kB 41.1MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.1->-r bpr-master/requirements.txt (line 2)) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.1->-r bpr-master/requirements.txt (line 2)) (2.8.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0->-r bpr-master/requirements.txt (line 4)) (1.12.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0->-r bpr-master/requirements.txt (line 4)) (7.0.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->-r bpr-master/requirements.txt (line 5)) (1.8.1)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->-r bpr-master/requirements.txt (line 5)) (19.3.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->-r bpr-master/requirements.txt (line 5)) (0.2.4)\n","Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->-r bpr-master/requirements.txt (line 5)) (1.6.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->-r bpr-master/requirements.txt (line 5)) (20.4)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest==5.3.5->-r bpr-master/requirements.txt (line 5)) (8.4.0)\n","Collecting pluggy<1.0,>=0.12\n","  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest==5.3.5->-r bpr-master/requirements.txt (line 5)) (3.1.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pytest==5.3.5->-r bpr-master/requirements.txt (line 5)) (2.4.7)\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: numpy, pandas, torch, torchvision, pluggy, pytest\n","  Found existing installation: numpy 1.18.5\n","    Uninstalling numpy-1.18.5:\n","      Successfully uninstalled numpy-1.18.5\n","  Found existing installation: pandas 1.0.4\n","    Uninstalling pandas-1.0.4:\n","      Successfully uninstalled pandas-1.0.4\n","  Found existing installation: torch 1.5.0+cu101\n","    Uninstalling torch-1.5.0+cu101:\n","      Successfully uninstalled torch-1.5.0+cu101\n","  Found existing installation: torchvision 0.6.0+cu101\n","    Uninstalling torchvision-0.6.0+cu101:\n","      Successfully uninstalled torchvision-0.6.0+cu101\n","  Found existing installation: pluggy 0.7.1\n","    Uninstalling pluggy-0.7.1:\n","      Successfully uninstalled pluggy-0.7.1\n","  Found existing installation: pytest 3.6.4\n","    Uninstalling pytest-3.6.4:\n","      Successfully uninstalled pytest-3.6.4\n","Successfully installed numpy-1.18.1 pandas-1.0.1 pluggy-0.13.1 pytest-5.3.5 torch-1.4.0 torchvision-0.5.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy","pandas"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"yT_TPDIxALXZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1592827676868,"user_tz":-120,"elapsed":3857,"user":{"displayName":"Costanza Siani","photoUrl":"","userId":"11236584817392907357"}},"outputId":"b9b4b420-8aa2-4c60-94aa-996e2ff1c994"},"source":["import os\n","import gzip\n","import json\n","import math\n","import random\n","import pickle\n","import pprint\n","import argparse\n","\n","import numpy as np\n","import pandas as pd\n","\n","\n","class DatasetLoader(object):\n","    def load(self):\n","        \"\"\"Minimum condition for dataset:\n","          * All users must have at least one item record.\n","          * All items must have at least one user record.\n","        \"\"\"\n","        raise NotImplementedError\n","\n","class Hetrec(DatasetLoader):\n","    def __init__(self, data_dir):\n","        self.fpath = os.path.join('/content/drive/My Drive/Colab Notebooks/Thesis/hetrec2011-lastfm-2k', 'user_artists.dat')\n","\n","    def load(self):\n","        # Load data\n","        df = pd.read_csv(self.fpath,\n","                         sep=\"\\s+\",\n","                         engine='python',\n","                         names=['userID', 'itemID', 'weight'])\n","        # TODO: Remove negative rating?\n","        # df = df[df['rate'] >= 3]\n","        return df\n","\n","# class MovieLens1M(DatasetLoader):\n","#     def __init__(self, data_dir):\n","#         self.fpath = os.path.join('/content/drive/My Drive/Colab Notebooks/Thesis/data_dir', 'ratings.dat')\n","\n","#     def load(self):\n","#         # Load data\n","#         df = pd.read_csv(self.fpath,\n","#                          sep='::',\n","#                          engine='python',\n","#                          names=['user', 'item', 'rate','time'])\n","#         # TODO: Remove negative rating?\n","#         # df = df[df['rate'] >= 3]\n","#         return df\n","\n","\n","# class MovieLens20M(DatasetLoader):\n","#     def __init__(self, data_dir):\n","#         self.fpath = os.path.join('Thesis/data_dir', 'ratings.dat')\n","\n","#     def load(self):\n","#         df = pd.read_csv(self.fpath,\n","#                          sep=',',\n","#                          names=['user', 'item', 'rate'],\n","#                          usecols=['user', 'item'],\n","#                          skiprows=1)\n","#         return df\n","\n","\n","# class AmazonBeauty(DatasetLoader):\n","#     def __init__(self, data_dir, file_name='All_Beauty.json.gz'):\n","#         self.fpath = os.path.join(data_dir, file_name)\n","\n","#     def load(self):\n","#         raw_list = []\n","#         with gzip.open(self.fpath) as f:\n","#             for idx, line in enumerate(f):\n","#                 raw_data = json.loads(line)\n","#                 raw_list.append({'user': raw_data['reviewerID'],\n","#                                  'item': raw_data['asin'],\n","#                                  'rate': raw_data['overall'],\n","#                                  'time': raw_data['unixReviewTime']})\n","#         df = pd.DataFrame(raw_list)\n","#         print('Check if any column has null value')\n","#         print(df.isnull().any())\n","#         print('Total user number: %d' % df['user'].nunique())\n","#         print('Total item number: %d' % df['item'].nunique())\n","#         print('The number of unique item per user')\n","#         print(df.groupby('user')['item'].nunique().value_counts())\n","#         print('The number of unique user per item')\n","#         print(df.groupby('item')['user'].nunique().value_counts())\n","#         return df\n","\n","# class Hetrec(DatasetLoader):\n","#     def __init__(self, data_dir, file_name='user_artists.dat'):\n","#         self.fpath = os.path.join(data_dir, file_name)\n","\n","#     def load(self):\n","#         raw_list = []\n","#         with gzip.open(self.fpath) as f:\n","#             for idx, line in enumerate(f):\n","#                 raw_data = json.loads(line)\n","#                 raw_list.append({'user': raw_data['userID'],\n","#                                  'item': raw_data['artistID'],\n","#                                  'rate': raw_data['weight'],})\n","#         df = pd.DataFrame(raw_list)\n","#         print('Check if any column has null value')\n","#         print(df.isnull().any())\n","#         print('Total user number: %d' % df['user'].nunique())\n","#         print('Total item number: %d' % df['item'].nunique())\n","#         print('The number of unique item per user')\n","#         print(df.groupby('user')['item'].nunique().value_counts())\n","#         print('The number of unique user per item')\n","#         print(df.groupby('item')['user'].nunique().value_counts())\n","#         return df\n","\n","# class Gowalla(DatasetLoader):\n","#     \"\"\"Work In Progress\"\"\"\n","#     def __init__(self, data_dir):\n","#         self.fpath = os.path.join(data_dir, 'loc-gowalla_totalCheckins.txt')\n","\n","#     def load(self):\n","#         df = pd.read_csv(self.fpath,\n","#                          sep='\\t',\n","#                          names=['user', 'time', 'latitude', 'longitude', 'item'],\n","#                          usecols=['user', 'item', 'time'])\n","#         df_size, df_nxt_size = 0, len(df)\n","#         while df_size != df_nxt_size:\n","#             # Update\n","#             df_size = df_nxt_size\n","\n","#             # Remove user which doesn't contain at least five items to guarantee the existance of `test_item`\n","#             groupby_user = df.groupby('user')['item'].nunique()\n","#             valid_user = groupby_user.index[groupby_user >= 15].tolist()\n","#             df = df[df['user'].isin(valid_user)]\n","#             df = df.reset_index(drop=True)\n","\n","#             # Remove item which doesn't contain at least five users\n","#             groupby_item = df.groupby('item')['user'].nunique()\n","#             valid_item = groupby_item.index[groupby_item >= 15].tolist()\n","#             df = df[df['item'].isin(valid_item)]\n","#             df = df.reset_index(drop=True)\n","\n","#             # Update\n","#             df_nxt_size = len(df)\n","\n","#         print('User distribution')\n","#         print(df.groupby('user')['item'].nunique().describe())\n","#         print('Item distribution')\n","#         print(df.groupby('item')['user'].nunique().describe())\n","#         return df\n","\n","\n","def convert_unique_idx(df, column_name):\n","    column_dict = {x: i for i, x in enumerate(df[column_name].unique())}\n","    # print(column_dict)\n","    df[column_name] = df[column_name].apply(column_dict.get)\n","    # df[column_name] = ''.join(map(str,df[column_name])) #converting the list into string\n","    # df[column_name] = np.float(df[column_name])\n","    # df[column_name] = df[column_name].astype('float')\n","    # df[column_name] = df[column_name].astype('int64')\n","    df[column_name] = round(df[column_name])\n","    # assert df[column_name].min() == 0\n","    # assert df[column_name].max() == len(column_dict) - 1\n","    return df, column_dict\n","\n","\n","def create_user_list(df, user_size):\n","    user_list = [list() for u in range(user_size)]\n","    for row in df.itertuples():\n","        user_list[row.userID].append(row.itemID)\n","    return user_list\n","\n","\n","def split_train_test(df, user_size, test_size=0.25, time_order=False):\n","    \"\"\"Split a dataset into `train_user_list` and `test_user_list`.\n","    Because it needs `user_list` for splitting dataset as `time_order` is set,\n","    Returning `user_list` data structure will be a good choice.\"\"\"\n","    # TODO: Handle duplicated items\n","    if not time_order:\n","        test_idx = np.random.choice(len(df), size=int(len(df)*test_size))\n","        train_idx = list(set(range(len(df))) - set(test_idx))\n","        test_df = df.loc[test_idx].reset_index(drop=True)\n","        train_df = df.loc[train_idx].reset_index(drop=True)\n","        test_user_list = create_user_list(test_df, user_size)\n","        train_user_list = create_user_list(train_df, user_size)\n","    else:\n","        total_user_list = create_user_list(df, user_size)\n","        train_user_list = [None] * len(user_list)\n","        test_user_list = [None] * len(user_list)\n","        for user, item_list in enumerate(total_user_list):\n","            # Choose latest item\n","            item_list = sorted(item_list, key=lambda x: x[0])\n","            # Split item\n","            test_item = item_list[math.ceil(len(item_list)*(1-test_size)):]\n","            train_item = item_list[:math.ceil(len(item_list)*(1-test_size))]\n","            # Register to each user list\n","            test_user_list[user] = test_item\n","            train_user_list[user] = train_item\n","    # # Remove time --> dont have time\n","    # test_user_list = [list(map(lambda x: x[1], l)) for l in test_user_list]\n","    # train_user_list = [list(map(lambda x: x[1], l)) for l in train_user_list]\n","    return train_user_list, test_user_list\n","\n","\n","def create_pair(user_list):\n","    pair = []\n","    for user, item_list in enumerate(user_list):\n","        pair.extend([(user, item) for item in item_list])\n","    return pair\n","\n","\n","def main():\n","    dataset = \"hetrec\"\n","    data_dir = \"hetrec2011-lastfm-2k\"\n","    output_data = os.path.join('preprocessed', 'hetrec.pickle')\n","    test_size = 0.25\n","    time_order = False\n","    \n","    if dataset == 'ml-1m':\n","        df = MovieLens1M(data_dir).load()\n","    elif dataset == 'hetrec':\n","        df = Hetrec(data_dir).load()\n","    elif dataset == 'ml-20m':\n","        df = MovieLens20M(data_dir).load()\n","    elif dataset == 'amazon-beauty':\n","        df = AmazonBeauty(data_dir).load()\n","    else:\n","        raise NotImplementedError\n","    df, user_mapping = convert_unique_idx(df, 'userID')\n","    df, item_mapping = convert_unique_idx(df, 'itemID')\n","    print('Complete assigning unique index to user and item')\n","\n","    user_size = len(df['userID'].unique())\n","    item_size = len(df['itemID'].unique())\n","\n","    train_user_list, test_user_list = split_train_test(df,\n","                                                       user_size,\n","                                                       test_size=test_size,\n","                                                       time_order=time_order)\n","    print('Complete spliting items for training and testing')\n","\n","    train_pair = create_pair(train_user_list)\n","    print('Complete creating pair')\n","\n","    dataset = {'user_size': user_size, 'item_size': item_size, \n","               'user_mapping': user_mapping, 'item_mapping': item_mapping,\n","               'train_user_list': train_user_list, 'test_user_list': test_user_list,\n","               'train_pair': train_pair}\n","    dirname = os.path.dirname(os.path.abspath(output_data))\n","    os.makedirs(dirname, exist_ok=True)\n","    with open(output_data, 'wb') as f:\n","        pickle.dump(dataset, f, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","if __name__ == '__main__':\n","    main()\n","    # # Parse argument\n","    # parser = argparse.ArgumentParser()\n","    # parser.add_argument('--dataset',\n","    #                     choices=['ml-1m', 'ml-20m', 'amazon-beauty', 'gowalla','hetrec'])\n","    # parser.add_argument('--data_dir',\n","    #                     type=str,\n","    #                     default=os.path.join('data', 'hetrec'),\n","    #                     help=\"File path for raw data\")\n","    # parser.add_argument('--output_data',\n","    #                     type=str,\n","    #                     default=os.path.join('preprocessed', 'hetrec.pickle'),\n","    #                     help=\"File path for preprocessed data\")\n","    # parser.add_argument('--test_size',\n","    #                     type=float,\n","    #                     default=0.25,\n","    #                     help=\"Proportion for training and testing split\")\n","\n","    # args = parser.parse_args()\n","    # main(args)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Complete assigning unique index to user and item\n","Complete spliting items for training and testing\n","Complete creating pair\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pv0htHEYAbCf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":765},"executionInfo":{"status":"ok","timestamp":1592828731755,"user_tz":-120,"elapsed":63217,"user":{"displayName":"Costanza Siani","photoUrl":"","userId":"11236584817392907357"}},"outputId":"67d313f3-f82e-494e-9031-a46cfd6cfed7"},"source":["import os\n","import random\n","import pickle\n","import argparse\n","from collections import deque\n","\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import IterableDataset, DataLoader, get_worker_info\n","from torch.utils.tensorboard import SummaryWriter\n","\n","\n","class TripletUniformPair(IterableDataset):\n","    def __init__(self, num_item, user_list, pair, shuffle, num_epochs):\n","        self.num_item = num_item\n","        self.user_list = user_list\n","        self.pair = pair\n","        self.shuffle = shuffle\n","        self.num_epochs = num_epochs\n","\n","    def __iter__(self):\n","        worker_info = get_worker_info()\n","        # Shuffle per epoch\n","        self.example_size = self.num_epochs * len(self.pair)\n","        self.example_index_queue = deque([])\n","        self.seed = 0\n","        if worker_info is not None:\n","            self.start_list_index = worker_info.id\n","            self.num_workers = worker_info.num_workers\n","            self.index = worker_info.id\n","        else:\n","            self.start_list_index = None\n","            self.num_workers = 1\n","            self.index = 0\n","        return self\n","\n","    def __next__(self):\n","        if self.index >= self.example_size:\n","            raise StopIteration\n","        # If `example_index_queue` is used up, replenish this list.\n","        while len(self.example_index_queue) == 0:\n","            index_list = list(range(len(self.pair)))\n","            if self.shuffle:\n","                random.Random(self.seed).shuffle(index_list)\n","                self.seed += 1\n","            if self.start_list_index is not None:\n","                index_list = index_list[self.start_list_index::self.num_workers]\n","                # Calculate next start index\n","                self.start_list_index = (self.start_list_index + (self.num_workers - (len(self.pair) % self.num_workers))) % self.num_workers\n","            self.example_index_queue.extend(index_list)\n","        result = self._example(self.example_index_queue.popleft())\n","        self.index += self.num_workers\n","        return result\n","\n","    def _example(self, idx):\n","        u = self.pair[idx][0]\n","        i = self.pair[idx][1]\n","        j = np.random.randint(self.num_item)\n","        while j in self.user_list[u]:\n","            j = np.random.randint(self.num_item)\n","        return u, i, j\n","\n","\n","class BPR(nn.Module):\n","    def __init__(self, user_size, item_size, dim, weight_decay):\n","        super().__init__()\n","        self.W = nn.Parameter(torch.empty(user_size, dim))\n","        self.H = nn.Parameter(torch.empty(item_size, dim))\n","        nn.init.xavier_normal_(self.W.data)\n","        nn.init.xavier_normal_(self.H.data)\n","        self.weight_decay = weight_decay\n","\n","    def forward(self, u, i, j):\n","        \"\"\"Return loss value.\n","        \n","        Args:\n","            u(torch.LongTensor): tensor stored user indexes. [batch_size,]\n","            i(torch.LongTensor): tensor stored item indexes which is prefered by user. [batch_size,]\n","            j(torch.LongTensor): tensor stored item indexes which is not prefered by user. [batch_size,]\n","        \n","        Returns:\n","            torch.FloatTensor\n","        \"\"\"\n","        u = self.W[u, :]\n","        i = self.H[i, :]\n","        j = self.H[j, :]\n","        x_ui = torch.mul(u, i).sum(dim=1)\n","        x_uj = torch.mul(u, j).sum(dim=1)\n","        x_uij = x_ui - x_uj\n","        log_prob = F.logsigmoid(x_uij).sum()\n","        regularization = self.weight_decay * (u.norm(dim=1).pow(2).sum() + i.norm(dim=1).pow(2).sum() + j.norm(dim=1).pow(2).sum())\n","        return -log_prob + regularization\n","\n","    def recommend(self, u):\n","        \"\"\"Return recommended item list given users.\n","        Args:\n","            u(torch.LongTensor): tensor stored user indexes. [batch_size,]\n","        Returns:\n","            pred(torch.LongTensor): recommended item list sorted by preference. [batch_size, item_size]\n","        \"\"\"\n","        u = self.W[u, :]\n","        x_ui = torch.mm(u, self.H.t())\n","        pred = torch.argsort(x_ui, dim=1)\n","        return pred\n","\n","\n","def precision_and_recall_k(user_emb, item_emb, train_user_list, test_user_list, klist, batch=512):\n","    \"\"\"Compute precision at k using GPU.\n","    Args:\n","        user_emb (torch.Tensor): embedding for user [user_num, dim]\n","        item_emb (torch.Tensor): embedding for item [item_num, dim]\n","        train_user_list (list(set)):\n","        test_user_list (list(set)):\n","        k (list(int)):\n","    Returns:\n","        (torch.Tensor, torch.Tensor) Precision and recall at k\n","    \"\"\"\n","    # Calculate max k value\n","    max_k = max(klist)\n","\n","    # Compute all pair of training and test record\n","    result = None\n","    for i in range(0, user_emb.shape[0], batch):\n","        # Create already observed mask\n","        mask = user_emb.new_ones([min([batch, user_emb.shape[0]-i]), item_emb.shape[0]])\n","        for j in range(batch):\n","            if i+j >= user_emb.shape[0]:\n","                break\n","            mask[j].scatter_(dim=0, index=torch.tensor(list(train_user_list[i+j])).cuda(), value=torch.tensor(0.0).cuda())\n","        # Calculate prediction value\n","        cur_result = torch.mm(user_emb[i:i+min(batch, user_emb.shape[0]-i), :], item_emb.t())\n","        cur_result = torch.sigmoid(cur_result)\n","        assert not torch.any(torch.isnan(cur_result))\n","        # Make zero for already observed item\n","        cur_result = torch.mul(mask, cur_result)\n","        _, cur_result = torch.topk(cur_result, k=max_k, dim=1)\n","        result = cur_result if result is None else torch.cat((result, cur_result), dim=0)\n","\n","    result = result.cpu()\n","    # Sort indice and get test_pred_topk\n","    precisions, recalls = [], []\n","    for k in klist:\n","        precision, recall = 0, 0\n","        for i in range(user_emb.shape[0]):\n","            test = set(test_user_list[i])\n","            pred = set(result[i, :k].numpy().tolist())\n","            val = len(test & pred)\n","            precision += val / max([min([k, len(test)]), 1])\n","            recall += val / max([len(test), 1])\n","        precisions.append(precision / user_emb.shape[0])\n","        recalls.append(recall / user_emb.shape[0])\n","    return precisions, recalls\n","\n","def main(): \n","    data = os.path.join('/content/drive/My Drive/Colab Notebooks/Thesis/preprocessed', 'hetrec.pickle')\n","    seed = 0\n","    dim = 4\n","    lr = 1e-3\n","    weight_decay = 0.025\n","    n_epochs = 40\n","    batch_size = 4096\n","    print_every = 20\n","    eval_every = 100\n","    save_every = 10000\n","    model = os.path.join('output', 'bpr.pt')\n","    \n","    # Initialize seed\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","    # Load preprocess data\n","    with open(data, 'rb') as f:\n","        dataset = pickle.load(f)\n","        user_size, item_size = dataset['user_size'], dataset['item_size']\n","        train_user_list, test_user_list = dataset['train_user_list'], dataset['test_user_list']\n","        train_pair = dataset['train_pair']\n","    print('Load complete')\n","\n","    # Create dataset, model, optimizer\n","    dataset = TripletUniformPair(item_size, train_user_list, train_pair, True, n_epochs)\n","    loader = DataLoader(dataset, batch_size=batch_size, num_workers=16)\n","    model = BPR(user_size, item_size, dim, weight_decay).cuda()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    writer = SummaryWriter()\n","\n","    # Training\n","    smooth_loss = 0\n","    idx = 0\n","    for u, i, j in loader:\n","        optimizer.zero_grad()\n","        loss = model(u, i, j)\n","        loss.backward()\n","        optimizer.step()\n","        writer.add_scalar('train/loss', loss, idx)\n","        smooth_loss = smooth_loss*0.99 + loss*0.01\n","        if idx % print_every == (print_every - 1):\n","            print('loss: %.4f' % smooth_loss)\n","            # print(idx)\n","            # print(idx % eval_every)\n","            # print(eval_every - 1)\n","        if idx % eval_every == (eval_every - 1):\n","            plist, rlist = precision_and_recall_k(model.W.detach(),\n","                                                    model.H.detach(),\n","                                                    train_user_list,\n","                                                    test_user_list,\n","                                                    klist=[1, 5, 10])\n","            print('P@1: %.4f, P@5: %.4f P@10: %.4f, R@1: %.4f, R@5: %.4f, R@10: %.4f' % (plist[0], plist[1], plist[2], rlist[0], rlist[1], rlist[2]))\n","            writer.add_scalars('eval', {'P@1': plist[0],\n","                                                    'P@5': plist[1],\n","                                                    'P@10': plist[2]}, idx)\n","            writer.add_scalars('eval', {'R@1': rlist[0],\n","                                                'R@5': rlist[1],\n","                                                'R@10': rlist[2]}, idx)\n","        if idx % save_every == (save_every - 1):\n","            dirname = os.path.dirname(os.path.abspath(model))\n","            os.makedirs(dirname, exist_ok=True)\n","            torch.save(model.state_dict(), model)\n","        idx += 1\n","        \n","if __name__ == '__main__':\n","  main()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Load complete\n","loss: 517.0642\n","loss: 939.9053\n","loss: 1285.6853\n","loss: 1568.3467\n","loss: 1799.0814\n","P@1: 0.0961, P@5: 0.0771 P@10: 0.0672, R@1: 0.0091, R@5: 0.0358, R@10: 0.0583\n","loss: 1986.6620\n","loss: 2137.7771\n","loss: 2257.7046\n","loss: 2351.0508\n","loss: 2421.0427\n","P@1: 0.1801, P@5: 0.1301 P@10: 0.1128, R@1: 0.0174, R@5: 0.0608, R@10: 0.0975\n","loss: 2470.8005\n","loss: 2502.5469\n","loss: 2519.3704\n","loss: 2523.1741\n","loss: 2515.5540\n","P@1: 0.1902, P@5: 0.1336 P@10: 0.1122, R@1: 0.0175, R@5: 0.0628, R@10: 0.0971\n","loss: 2499.3455\n","loss: 2474.8379\n","loss: 2444.5378\n","loss: 2408.9631\n","loss: 2368.8203\n","P@1: 0.1823, P@5: 0.1291 P@10: 0.1096, R@1: 0.0169, R@5: 0.0605, R@10: 0.0948\n","loss: 2325.8193\n","loss: 2282.6333\n","loss: 2238.3979\n","loss: 2193.4138\n","loss: 2150.3726\n","P@1: 0.1828, P@5: 0.1241 P@10: 0.1080, R@1: 0.0169, R@5: 0.0579, R@10: 0.0933\n","loss: 2107.9331\n","loss: 2067.7373\n","loss: 2027.8967\n","loss: 1991.0612\n","loss: 1956.2156\n","P@1: 0.1722, P@5: 0.1214 P@10: 0.1074, R@1: 0.0160, R@5: 0.0568, R@10: 0.0928\n","loss: 1921.9966\n","loss: 1891.8849\n","loss: 1859.6427\n","loss: 1830.8423\n","loss: 1804.1311\n","P@1: 0.1812, P@5: 0.1257 P@10: 0.1113, R@1: 0.0168, R@5: 0.0584, R@10: 0.0962\n","loss: 1564.1544\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zgmyLG6WCXiC","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}